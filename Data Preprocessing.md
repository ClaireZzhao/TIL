# ë°ì´í„° ì „ì²˜ë¦¬(Data Preprocessing)

## ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì˜ˆì¸¡ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ë°ì´í„° ì „ì²˜ë¦¬ê°€ í•„ìš”í•¨

### 1. ê²°ì¸¡ì¹˜(NaN) ì²˜ë¦¬:  ê²°ì¸¡ì¹˜ í™•ì¸ & ì²˜ë¦¬(ì œê±° ë˜ëŠ” ëŒ€ì²´)

- ê²°ì¸¡ì¹˜ í™•ì¸

```python
df.isnull().any()  # ì „ì²´ ì¹¼ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ìœ ë¬´ í™•ì¸: default: axis = 0(ì—´)
df.isnull().any(axis=1) # ì „ì²´ í–‰ ê¸°ì¤€ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ìœ ë¬´ í™•ì¸: return true/false

df.isnull().sum()  # ê²°ì¸¡ì¹˜ ê°œìˆ˜ í™•ì¸ 
```

- ê²°ì¸¡ì¹˜ ì œê±°: ê²°ì¸¡ì¹˜ê°€ ì ì€ ê²½ìš°

```python
dropna = df.dropna(subset=['ì¹¼ëŸ¼ëª…']) # ê²°ì¸¡ì¹˜ í¬í•¨ í–‰ ì œê±°(íŠ¹ì •ì¹¼ëŸ¼ ê¸°ì¤€)
dropna2 = df.dropna()  # ê²°ì¸¡ì¹˜ í¬í•¨ í–‰ ì œê±°(ì „ì²´ ì¹¼ëŸ¼ ê¸°ì¤€)
```

- ê²°ì¸¡ì¹˜ ëŒ€ì²´: ê²°ì¸¡ì¹˜ê°€ ë§ì€ ê²½ìš°

```python
df['ì¹¼ëŸ¼ëª…'] = df['ì¹¼ëŸ¼ëª…'].fillna(0)  # 0ìœ¼ë¡œ ëŒ€ì²´
df['ì¹¼ëŸ¼ëª…'].fillna(df['ì¹¼ëŸ¼ëª…'].mean(), inplace=True) # í‰ê· ìœ¼ë¡œ ëŒ€ì²´

data_null_value = data.fillna(value={'Horsepower': data['Horsepower'].mode()[0],
                           'Miles_per_Gallon': data['Miles_per_Gallon'].mean()}, 
                            inplace = False) # ì—¬ëŸ¬ ì¹¼ëŸ¼ì— ìˆëŠ” ê²°ì¸¡ì¹˜ ëŒ€ì²´
```

<aside>
ğŸ‘‰ ì¤‘ë³µëœ ê°’ì´ ìˆëŠ” ê²½ìš°

</aside>

```python
data.duplicated()  # ì¤‘ë³µëœ ê°’ì´ ìˆìœ¼ë©´ Trueë¡œ, ì•„ë‹ˆë©´ Falseë¡œ return 
data[data.duplicated()] # ìƒ‰ì¸ìœ¼ë¡œ íƒìƒ‰
data.duplicated().sum() # ì¤‘ë³µëœ ê°’ì€ ëª‡ê°œê°€ ìˆëŠ”ì§€ return

data.drop_duplicates() # ì²«ë²ˆì§¸ ê°’ì„ ì œì™¸í•œ ì¤‘ë³µ ê°’ì„ ì œê±°(ì „ì²´ ì¹¼ëŸ¼ ê¸°ì¤€)
data.drop_duplicates(subset=['Horsepower', 'Miles_per_Gallon'], keep='first', 
                     inplace = False) # íŠ¹ì • ì¹¼ëŸ¼ ê°€ì¤€
```

### 2. ì´ìƒì¹˜(outlier) ì²˜ë¦¬: ì´ìƒì¹˜ íƒìƒ‰ & ì´ìƒì¹˜ ì²˜ë¦¬(ì œê±° ë˜ëŠ” ëŒ€ì²´)

- ì´ìƒì¹˜: ì •ìƒë²”ì£¼ì—ì„œ ë²—ì–´ë‚œ ê°’(ê·¹ë‹¨ì ìœ¼ë¡œ í¬ê±°ë‚˜ ì‘ì€ ê°’)
- ì •ìƒë²”ì£¼ë¥¼ ì–´ë–»ê²Œ í™•ì¸í•˜ëŠ”ê°€?
    
    <aside>
    ğŸ‘‰ í•˜í•œê°’ = í‰ê·  - n*í‘œì¤€í¸ì°¨ (n = 3: threshold)
    ìƒí•œê°’ = í‰ê·  + n*í‘œì¤€í¸ì°¨ (n = 3: threshold)    0
    
    </aside>
    
    <aside>
    ğŸ‘‰ minval = Q1 -  IQR * 1.5         # IQR = Q3 - Q1 
    maxval = Q3 + IQR * 1.5
    
    </aside>
    
- ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ê²½ìš°:

```python
# ì´ìƒì¹˜ í™•ì¸
dataset.gender.unique() # unique()í•¨ìˆ˜ë¡œ í™•ì¸
dataset.gender.value_counts() # value_counts()í•¨ìˆ˜ë¡œ í™•ì¸
plt.pie(dataset.gender.value_counts()) # pie chartë¡œ í™•ì¸
plt.show()
```

```python
# ì´ìƒì¹˜ ì²˜ë¦¬:
data = data[data.gender == 1 | data.gender == 2] # subsetí†µí•´ì„œ ì´ìƒì¹˜ ì œê±°

# ì´ìƒì¹˜ ì œê±°: ê´€ì¸¡ì¹˜ê°€ ì ê³ , í•´ë‹¹ ë³€ìˆ˜ì˜ ì˜ë¯¸ë¥¼ ì¼ê³  ìˆëŠ” ê²½ìš°
```

- ì—°ì†í˜• ë³€ìˆ˜ì˜ ê²½ìš°:

```python
# ìš”ì•½í†µê³„ëŸ‰ ë³´ê¸°
data.describe() # ìµœëŒ“ê°’ ë¬¸ì œ or ìŒìˆ˜ê°’ ë¬¸ì œ ë“± í™•ì¸ ê°€ëŠ¥

# boxplot ì´ìš© ì´ìƒì¹˜ íƒìƒ‰
import matplotlib.pyplot as plt

plt.boxplot(data['ì¹¼ëŸ¼ëª…'])
plt.show()

# ì •ìƒë²”ìœ„ë¥¼ êµ¬í•œ ê²½ìš°(IQR ìˆ˜ì‹ ë“± ì´ìš©í•˜ì—¬)
data[(data['age'] < minval) | (data['age'] > maxval)] # ì´ìƒì¹˜ í™•ì¸ 
```

```python
# ì´ìƒì¹˜ ì œê±°
new_data = data[data.bmi > 0] # subset ì´ìš©

# ì´ìƒì¹˜ ëŒ€ì²´: ì •ìƒë²”ì£¼ì˜ í•˜í•œê°’ê³¼ ìƒí•œê°’ìœ¼ë¡œ ëŒ€ì²´
X.loc[X.Fare > maxval, 'Fare'] = maxval # ìƒí•œê°’ìœ¼ë¡œ ëŒ€ì²´
```

<aside>
ğŸ‘‰ model ë§Œë“œëŠ”ë° ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ê±°ë‚˜ ëŒ€ì²´í•˜ë©´ ë˜ëŠ”ë°, 
ì—…ë¬´ì— ìˆì–´ì„œ ì´ìƒì¹˜ê°€ ë§ì€ ê°€ì¹˜ë“¤ì„ ê°€ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ìƒì¹˜ ì²˜ë¦¬í•˜ê¸° ì „ì— í™•ì¸ í•„ìš”í•¨

</aside>

### 3. ë°ì´í„° ì¸ì½”ë”©(data encoding)

- ë°ì´í„° ì¸ì½”ë”© ëŒ€ìƒ: machine learning modelì—ì„œ **ë²”ì£¼í˜• ë³€ìˆ˜** ëŒ€ìƒìœ¼ë¡œ ìˆ«ìí˜•ì˜ ëª©ë¡ìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” ì „ì²˜ë¦¬ ì‘ì—…
- ë°ì´í„° ì¸ì½”ë”© ë°©ë²•: label encoding, one-hot encoding
    - label encoding: yë³€ìˆ˜ or íŠ¸ë¦¬ëª¨ë¸ ê³„ì—´ì˜ xë³€ìˆ˜ ëŒ€ìƒ(ex: no/yes â†’ 0/1) -  10ì§„ìˆ˜ë¡œ ì¸ì½”ë”©
        
        ```python
        from sklearn.preprocessing import LabelEncoder
        
        encoder = LabelEncoder() # ê°ì²´ ìƒì„±(Init)
        labels = encoder.fit_transform(df['ì¹¼ëŸ¼ëª…']) # ì˜ë¬¸ì ì˜¤ë¦„ì°¨ìˆœ
        ```
        
    - one-hot encoding: íšŒê·€ëª¨ë¸, SVM ê³„ì—´ì˜ xë³€ìˆ˜ ëŒ€ìƒ - 2ì§„ìˆ˜(ë”ë¯¸ë³€ìˆ˜)ë¡œ ì¸ì½”ë”©
        
        ** íšŒê·€ëª¨ë¸ì—ì„œ ì¸ì½”ë”©ê°’ì´ ê°€ì¤‘ì¹˜ë¡œ ì ìš©ë˜ë¯€ë¡œ xë³€ìˆ˜ë¥¼ one-hot encodingë¡œ ë³€í™˜
        
        ```python
        import pandas as pd
        df_dummy = pd.get_dummies(data=df) #kê°œ ê°€ë³€ìˆ˜(ë”ë¯¸ë³€ìˆ˜)ìƒì„±-ê¸°ì¤€ë³€ìˆ˜(base)í¬í•¨
        df_dummy2 = pd.get_dummies(data=df, drop_first=True) # base ì œì™¸**(ê¶Œì¥)**
        
        #ìˆ«ìí˜•ë³€ìˆ˜ëŠ” ìë™ìœ¼ë¡œ ì œì™¸ë¨(objectí˜•ë³€ìˆ˜ ëŒ€ìƒë§Œ)->ìˆ«ìí˜•ë³€ìˆ˜ ëŒ€ìƒìœ¼ë¡œ encoding í•„ìš”ì‹œ
        df_dummy3 = pd.get_dummies(data=df, drop_first=True, 
                                   columns = ['ì¹¼ëŸ¼ëª…1', 'ì¹¼ëŸ¼ëª…2', 'ì¹¼ëŸ¼ëª…3']) 
        
        # ë”ë¯¸ë³€ìˆ˜ì˜ baseë¥¼ ë³€ê²½í•˜ê³ ì í•  ë•Œ
        # 1) objectí˜• -> categoryí˜• ë³€í™˜
        iris['species'] = iris['species'].astype('category')
        iris['species'] = iris['species'].cat.set_categories(['versicolor',
                                                              'virginica',
                                                              'sentosa'])
        iris_dummy = pd.get_dummies(data=iris, columns=['species'],
                                    drop_first=True)
        ```
        

### 4. í”¼ì²˜ ìŠ¤ì¼€ì¼ë§(feature scaling)

- Xë³€ìˆ˜(feature) ëŒ€ìƒ: ì„œë¡œ ë‹¤ë¥¸ í¬ê¸°(ë‹¨ìœ„)ë¥¼ ì¼ì •í•œ ë²”ìœ„ë¡œ ì¡°ì •í•˜ëŠ” ì „ì²˜ë¦¬ ì‘ì—…
- ë°©ë²•: í‘œì¤€í™”, ìµœì†Œ-ìµœëŒ€ ì •ê·œí™”, ë¡œê·¸ë³€í™˜
    - í‘œì¤€í™”(StandardScaler): xë³€ìˆ˜ ëŒ€ìƒìœ¼ë¡œ ì •ê·œë¶„í¬ê°€ ë  ìˆ˜ ìˆë„ë¡ í‰ê· =0, í‘œì¤€í¸ì°¨=1ë¡œ í†µì¼
        
        <aside>
        ğŸ‘‰ íšŒê·€ëª¨ë¸, SVMê³„ì—´ì— ì ìš©: xë³€ìˆ˜ê°€ ì •ê·œë¶„í¬ë¼ê³  ê°€ì •í•˜ì— í•™ìŠµ ì§„í–‰
        
        </aside>
        
        ```python
        from sklearn.preprocessing import scale # í‘œì¤€í™”(mu=1, std=1)
        x_zscore = scale(x)
        
        from sklearn.preprocessing import StandardScaler
        x_scaled = StandardScaler().fit_transform(X=iris.drop('species', axis=1))
        ```
        
    - ìµœì†Œ-ìµœëŒ€ ì •ê·œí™”(MinMaxScaler): xë³€ìˆ˜ ëŒ€ìƒìœ¼ë¡œ ìµœì†Ÿê°’=0, ìµœëŒ“ê°’=1ë¡œ  í†µì¼
        
        <aside>
        ğŸ‘‰ íŠ¸ë¦¬ëª¨ë¸ ê³„ì—´(íšŒê·€ëª¨ë¸ ê³„ì—´ì´ ì•„ë‹Œ ê²½ìš°)ì— ì ìš©
        
        </aside>
        
        ```python
        from sklearn.preprocessing import minmax_scale # ì •ê·œí™”(0~1)
        x_nor = minmax_scale(x)
        
        from sklearn.preprocessing import MinMaxScaler
        x_scaled = MinMaxScaler().fit_transform(X=iris.drop('species', axis=1))
        ```
        
    - ë¡œê·¸ë³€í™˜(log-transformation): log()í•¨ìˆ˜ë¡œ ë¹„ì„ í˜•ì„ ì„ í˜•ìœ¼ë¡œ, ì™œê³¡ì„ ê°–ëŠ” ë¶„í¬ë¥¼ ì£„ìš°ëŒ€ì¹­ í˜•íƒœì˜ ì •ê·œë¶„í¬ë¡œ ë³€í™˜
        
        <aside>
        ğŸ‘‰ íšŒê·€ëª¨ë¸ì—ì„œ yë³€ìˆ˜ ì ìš©
        
        </aside>
        
        ```python
        import numpy as np
        x_log = np.log(x)
        x_log1 = np.log1p(x) # xë³€ìˆ˜ì— 0ì´ ìˆëŠ” ê²½ìš°(warningë©”ì‹œì§€)
        x_log2 = np.log(np.abs(x)) # xë³€ìˆ˜ì— ìŒìˆ˜ê°€ ìˆëŠ” ê²½ìš°(warningë©”ì‹œì§€)
        ```
        
- ì–´ë–¤ ë°©ë²•ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§ í•˜ë©´ ì¢‹ì€ì§€?
    
    ```python
    from sklearn.preprocessing import minmax_scale, scale
    import numpy as np
    
    def scaling(X, y, kind='none') : 
        # xë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§  
        if kind == 'minmax_scale' :  
            X_trans = minmax_scale(X) 
        elif kind == 'zscore' : 
            X_trans = scale(X)  # zscore(X)
        elif kind == 'log' :  
            X_trans = np.log1p(np.abs(X)) 
        else :
            X_trans = X 
        
        # yë³€ìˆ˜ ë¡œê·¸ë³€í™˜ 
        if kind != 'none' :
            y = np.log1p(np.abs(y))   
        
        # train/test split 
        X_train,X_test,y_train,y_test = train_test_split(
            X_trans, y, test_size = 30, random_state=1)   
        
        print(f"scaling ë°©ë²• : {kind}, X í‰ê·  = {X_trans.mean()}")
        return X_train,X_test,y_train, y_test
    
    # í•¨ìˆ˜ í˜¸ì¶œ 
    #X_train,X_test,y_train,y_test = scaling(X, y,'none')
    #X_train,X_test,y_train,y_test = scaling(X, y,'minmax_scale')
    #X_train,X_test,y_train,y_test = scaling(X, y,'zscore')
    X_train,X_test,y_train,y_test = scaling(X, y,'log')
    
    # model ìƒì„±í•˜ê¸°
    model = LinearRegression().fit(X=X_train, y=y_train)  
    
    # model í‰ê°€í•˜ê¸°
    model_train_score = model.score(X_train, y_train) 
    model_test_score = model.score(X_test, y_test) 
    print('model train score =', model_train_score)
    print('model test score =', model_test_score)
    
    y_pred = model.predict(X_test)
    y_true = y_test
    print('R2 score =',r2_score(y_true, y_pred))  
    mse = mean_squared_error(y_true, y_pred)
    print('MSE =', mse)
    
    '''
    1. ê¸°ë³¸: x, yë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§ ì „
    scaling ë°©ë²• : none, X í‰ê·  = 70.07396704469443
    model train score = 0.7410721208614651
    model test score = 0.7170463430870563
    R2 score = 0.7170463430870563
    MSE = 20.20083182974776
    
    2. X: ì •ê·œí™”, y: ë¡œê·¸ë³€í™˜
    scaling ë°©ë²• : minmax_scale, X í‰ê·  = 0.3862566314283195
    model train score = 0.7910245321591743
    model test score = 0.7633961405434472
    R2 score = 0.7633961405434472
    MSE = 0.027922682660046286
    [í•´ì„¤] ì •í™•ë„ í–¥ìƒ, MSE: 0 ìˆ˜ë ¹ì •ë„ í‰ê°€
    
    3. X: í‘œì¤€í™”, y: ë¡œê·¸ë³€í™˜
    scaling ë°©ë²• : zscore, X í‰ê·  = -1.1147462804871136e-15
    model train score = 0.7910245321591743
    model test score = 0.7633961405434472
    R2 score = 0.7633961405434472
    MSE = 0.027922682660046282
    [í•´ì„¤] ì •ê·œí™”ì™€ ì •í™•ë„ ê²°ê³¼ëŠ” ë™ì¼í•¨
    
    4. X: ë¡œê·¸ë³€í™˜, y: ë¡œê·¸ë³€í™˜
    scaling ë°©ë²• : log, X í‰ê·  = 2.408779096889065
    model train score = 0.7971243680501752 (test scoreë³´ë‹¤ ë” ë†’ì•„ì•¼ í•˜ëŠ”ë°..)
    model test score = 0.8217362767578845
    R2 score = 0.8217362767578845
    MSE = 0.021037701520680126
    [í•´ì„¤] test scoreê°€ ë†’ì€ ë¶ˆì™„ì „í•œ ê²°ê³¼
    '''
    ```